{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DV6yMOCu-Vy"
   },
   "source": [
    "Nama : Wicaksana Julius Three\n",
    "NIM :41820010079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16cBgsr2tw1c",
    "outputId": "9e3bc876-1b95-4218-c148-9e7af9a3415a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.9.0-py2.py3-none-any.whl (277 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.2/277.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Twisted>=18.9.0 (from scrapy)\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=3.4.6 (from scrapy)\n",
      "  Downloading cryptography-41.0.1-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
      "  Downloading pyOpenSSL-23.2.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.2)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.6.3)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.27.1)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.12.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.12)\n",
      "Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, cryptography, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-15.1.0 cryptography-41.0.1 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.2.1 pyOpenSSL-23.2.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.9.0 service-identity-23.1.0 tldextract-3.4.4 w3lib-2.1.1 zope.interface-6.0\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Y1KjkFYRnU2V"
   },
   "outputs": [],
   "source": [
    "from scrapy  import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "I3cZ4mW8pvzf"
   },
   "outputs": [],
   "source": [
    "# Create the XPath string equivalent to the CSS Locator\n",
    "xpath = '/html/body/span[1]//a'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'html > body > span:nth-of-type(1) a'\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator\n",
    "xpath = '//div[@id=\"uid\"]/span//h4'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'div#uid > span h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rZdRHouNp7IX"
   },
   "outputs": [],
   "source": [
    "def how_many_elements( css ):\n",
    "    sel = Selector( text = data )\n",
    "    print( len(sel.css( css )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "a9GXCArTp-Bn"
   },
   "outputs": [],
   "source": [
    "with open('all.html', 'r') as file:\n",
    "    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npnX1R4Jp_r4",
    "outputId": "137e4d5c-2f0d-4516-ce2a-f91bcbadf55d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Create a selector from the html (of a secret website)\n",
    "sel = Selector(text=data)\n",
    "\n",
    "# Fill in the blank\n",
    "css_locator = ' div.course-block > a'\n",
    "\n",
    "# Print the number of selected elements\n",
    "how_many_elements(css_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_pXtiGFqBdy",
    "outputId": "9e3db8d0-89c9-40b1-e45a-8681c96eac0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "css_locator = '#uid > *'\n",
    "\n",
    "# Print the number of selected elements\n",
    "how_many_elements(css_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "g-oX9sYhqFGM"
   },
   "outputs": [],
   "source": [
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css('div.course-block > a')\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dGsH150SuWc-"
   },
   "outputs": [],
   "source": [
    "def our_xpath( xpath ):\n",
    "    xextr = sel.xpath( xpath ).extract()\n",
    "    return xextr\n",
    "\n",
    "def our_css( css ):\n",
    "    cextr = sel.css( css ).extract()\n",
    "    return cextr\n",
    "\n",
    "def print_results( xpath, css_locator ):\n",
    "    print( \"Your XPath extracts to following:\")\n",
    "    print( our_xpath(xpath) )\n",
    "    print(\"_________________\\n\")\n",
    "    print( \"Your CSS Locator extracts the following:\")\n",
    "    print( our_css(css_locator) )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h779_DC7uXRG",
    "outputId": "3bdf555b-1cd0-4b0b-b478-a8794f4553ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your XPath extracts to following:\n",
      "['Here is the ', ' link you want!']\n",
      "_________________\n",
      "\n",
      "Your CSS Locator extracts the following:\n",
      "['Here is the ', ' link you want!']\n"
     ]
    }
   ],
   "source": [
    "with open('text_extract.html', 'r') as file:\n",
    "    html = file.read().replace('\\n', '')\n",
    "\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]/text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text\n",
    "css_locator = 'p#p3::text'\n",
    "\n",
    "# Print the text from our selection\n",
    "print_results(xpath, css_locator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnFpGIdAuYrm",
    "outputId": "fa90602e-c8d5-460d-c06f-f8df1ca73950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your XPath extracts to following:\n",
      "['Here is the ', 'DataCamp', ' link you want!']\n",
      "_________________\n",
      "\n",
      "Your CSS Locator extracts the following:\n",
      "['Here is the ', 'DataCamp', ' link you want!']\n"
     ]
    }
   ],
   "source": [
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]//text()'\n",
    "\n",
    "# Create an CSS Locator string to the desired text\n",
    "css_locator = 'p#p3 ::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results(xpath, css_locator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eFNczaseuY0u"
   },
   "outputs": [],
   "source": [
    "def print_url_title( url, title ):\n",
    "    print( \"Here is what you found:\" )\n",
    "    print( \"\\t-URL: %s\" % url )\n",
    "    print( \"\\t-Title: %s\" % title )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "t65Mlq3juZta"
   },
   "outputs": [],
   "source": [
    "from scrapy.http.response.text import TextResponse\n",
    "\n",
    "response = TextResponse(url='https://www.datacamp.com/courses/all', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dT3CFvJlu3Gd",
    "outputId": "0028e14c-c66b-424c-adb7-8b780c55b6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is what you found:\n",
      "\t-URL: https://www.datacamp.com/courses/all\n",
      "\t-Title: None\n"
     ]
    }
   ],
   "source": [
    "# GEt the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title or the website loaded in response\n",
    "this_title = response.xpath('//title/text()').extract_first()\n",
    "\n",
    "# Print out our findings\n",
    "print_url_title(this_url, this_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3aYXzYDuZ4a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
